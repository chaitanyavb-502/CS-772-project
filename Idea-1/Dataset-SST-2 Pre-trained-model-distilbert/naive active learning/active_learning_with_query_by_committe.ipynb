{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "    This script implements Active Learning using the Query by Committee (QBC) strategy.\n",
    "    A committee of models is trained on different bootstrap samples of the labeled data.\n",
    "    For the unlabeled pool, we measure the committee's disagreement via vote entropy and select\n",
    "    the most uncertain samples for labeling. The model's performance on the validation set is\n",
    "    evaluated (using majority vote over the committee), and results are plotted over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 2  \n",
    "\n",
    "def create_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
    "val_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [item[\"sentence\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "    tokenized_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_label_size = 50\n",
    "acquisition_size = 50\n",
    "max_iterations = 10\n",
    "committee_size = 3  # Number of models in the committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "labeled_indices = all_indices[:initial_label_size]\n",
    "unlabeled_indices = all_indices[initial_label_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sizes = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Learning Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Active Learning Iteration 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Committee Member 1, Epoch 1: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 1, Epoch 1 Loss: 0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Committee Member 1, Epoch 2: 100%|██████████| 4/4 [00:13<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 1, Epoch 2 Loss: 0.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Committee Member 2, Epoch 1: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 2, Epoch 1 Loss: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Committee Member 2, Epoch 2: 100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 2, Epoch 2 Loss: 0.6286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Committee Member 3, Epoch 1: 100%|██████████| 4/4 [00:09<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 3, Epoch 1 Loss: 0.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Committee Member 3, Epoch 2: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee Member 3, Epoch 2 Loss: 0.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Committee on Validation Set: 100%|██████████| 28/28 [01:35<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Committee Majority Vote): 50.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Vote Entropy for Unlabeled Samples:  47%|████▋     | 980/2104 [3:28:41<3:59:21, 12.78s/it]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m committee:\n\u001b[0;32m     80\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     83\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:883\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    881\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 883\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    893\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:703\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:464\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    456\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    457\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    458\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m         output_attentions,\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:408\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    405\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    409\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    411\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:343\u001b[0m, in \u001b[0;36mFFN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:346\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 346\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(max_iterations):\n",
    "    print(f\"\\n=== Active Learning Iteration {iteration+1} ===\")\n",
    "    \n",
    "    # --- Committee Training ---\n",
    "    committee = []\n",
    "    for i in range(committee_size):\n",
    "    \n",
    "        model = create_model().to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        \n",
    "        bootstrap_indices = np.random.choice(labeled_indices, size=len(labeled_indices), replace=True)\n",
    "        bootstrap_subset = Subset(train_dataset, bootstrap_indices)\n",
    "        bootstrap_loader = DataLoader(bootstrap_subset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        model.train()\n",
    "        epochs = 2\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(bootstrap_loader, desc=f\"Training Committee Member {i+1}, Epoch {epoch+1}\"):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Committee Member {i+1}, Epoch {epoch+1} Loss: {epoch_loss/len(bootstrap_loader):.4f}\")\n",
    "        \n",
    "        committee.append(model)\n",
    "\n",
    "    # --- Evaluation on the Validation Set using Committee Majority Vote ---\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    all_votes = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating Committee on Validation Set\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            batch_votes = []  \n",
    "            \n",
    "            for model in committee:\n",
    "                model.eval()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                batch_votes.append(preds.cpu().numpy())\n",
    "            \n",
    "            batch_votes = np.array(batch_votes).T  # shape: (batch_size, committee_size)\n",
    "            \n",
    "            for votes in batch_votes:\n",
    "                vote_counts = Counter(votes)\n",
    "                majority_vote = vote_counts.most_common(1)[0][0]\n",
    "                all_votes.append(majority_vote)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = np.mean(np.array(all_votes) == np.array(all_labels))\n",
    "    print(f\"Validation Accuracy (Committee Majority Vote): {accuracy*100:.2f}%\")\n",
    "    labeled_sizes.append(len(labeled_indices))\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    # --- Acquisition: Query by Committee using Vote Entropy ---\n",
    "    if len(unlabeled_indices) == 0:\n",
    "        break\n",
    "\n",
    "    unlabeled_subset = Subset(train_dataset, unlabeled_indices)\n",
    "    unlabeled_loader = DataLoader(unlabeled_subset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    vote_entropy_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(unlabeled_loader, desc=\"Computing Vote Entropy for Unlabeled Samples\")):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            committee_preds = []\n",
    "            for model in committee:\n",
    "                model.eval()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                committee_preds.append(preds.cpu().numpy())\n",
    "            \n",
    "            committee_preds = np.array(committee_preds)  # shape: (committee_size, batch_size)\n",
    "            \n",
    "            batch_size = committee_preds.shape[1]\n",
    "            for j in range(batch_size):\n",
    "                votes = committee_preds[:, j]  \n",
    "                vote_counts = Counter(votes)\n",
    "                \n",
    "                entropy = 0.0\n",
    "                for cls in range(num_labels):\n",
    "                    p = vote_counts.get(cls, 0) / committee_size\n",
    "                    if p > 0:\n",
    "                        entropy -= p * math.log(p)\n",
    "                \n",
    "                global_index = unlabeled_indices[batch_idx * 32 + j]\n",
    "                vote_entropy_list.append((global_index, entropy))\n",
    "    \n",
    "    vote_entropy_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    selected_samples = [idx for idx, ent in vote_entropy_list[:acquisition_size]]\n",
    "    \n",
    "    labeled_indices.extend(selected_samples)\n",
    "    unlabeled_indices = list(set(unlabeled_indices) - set(selected_samples))\n",
    "    \n",
    "    print(f\"Total labeled samples after acquisition: {len(labeled_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(labeled_sizes, [acc * 100 for acc in val_accuracies], marker='o', linestyle='-')\n",
    "plt.xlabel(\"Number of Labeled Samples\")\n",
    "plt.ylabel(\"Validation Accuracy (%)\")\n",
    "plt.title(\"Active Learning Performance (Query by Committee)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
